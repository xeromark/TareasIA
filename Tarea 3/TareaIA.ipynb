{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMRuVLldW8OlH6Xazc0h6OP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xeromark/TareasIA/blob/main/Tarea%203/TareaIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tarea 3 de IA de Omar Marca y Luis Reyes. Github: https://github.com/xeromark/TareasIA\n",
        "\n"
      ],
      "metadata": {
        "id": "DJLWWciOcCSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 1"
      ],
      "metadata": {
        "id": "k80rj0X2bOmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reproduzca el tutorial implementando la arquitectura. Comente paso a paso lo que se hace en sus propias palabras."
      ],
      "metadata": {
        "id": "G-CM64tLbZVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las neuronas KAN utilizan funciones de borde no lineales para transformar las entradas con el objetivo de que se pueda procesar la información con menos recursos computacionales y realizar cálculos más complejos.\n",
        "\n",
        "\n",
        "\n",
        "Por otra parte, en MLP las entradas se transforman mediante funciones de activación lineales o no lineales para introducir no linealidades en el modelo, es decir, que el modelo sea capaz de representar relaciones complejas entre las variables de entrada y salida que no pueden ser descritas por una simple relación lineal.\n",
        "\n",
        "\n",
        "Para poder utilizar MLP y KAN, se establecen métodos en python para proporcionar una estructura versátil que pueda acomodar tanto las Redes de Kolmogorov-Arnold (KAN) como los Perceptrones Multicapa (MLP). Es por ello que se tienen los siguientes métodos del funcionamiento de una neurona:\n"
      ],
      "metadata": {
        "id": "aRWSl4O4b6Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "    def __init__(self, n_in, n_weights_per_edge, weights_range=None):\n",
        "        self.n_in = n_in  # n. inputs\n",
        "        self.n_weights_per_edge = n_weights_per_edge\n",
        "        weights_range = [-1, 1] if weights_range is None else weights_range\n",
        "        self.weights = np.random.uniform(weights_range[0], weights_range[-1], size=(self.n_in, self.n_weights_per_edge))\n",
        "        self.bias = 0\n",
        "        self.xin = None  # input variable\n",
        "        self.xmid = None  # edge variables\n",
        "        self.xout = None  # output variable\n",
        "        self.dxout_dxmid = None  # derivative d xout / d xmid: (n_in, )\n",
        "        self.dxout_dbias = None  # derivative d xout / d bias\n",
        "        self.dxmid_dw = None  # derivative d xmid / d w: (n_in, n_par_per_edge)\n",
        "        self.dxmid_dxin = None  # derivative d xmid / d xin\n",
        "        self.dxout_dxin = None  # (composite) derivative d xout / d xin\n",
        "        self.dxout_dw = None  # (composite) derivative d xout / d w\n",
        "        self.dloss_dw = np.zeros((self.n_in, self.n_weights_per_edge))  # (composite) derivative d loss / d w\n",
        "        self.dloss_dbias = 0  # (composite) derivative d loss / d bias\n",
        "\n",
        "    def __call__(self, xin):\n",
        "        # forward pass: compute neuron's output\n",
        "        self.xin = np.array(xin)\n",
        "        self.get_xmid()\n",
        "        self.get_xout()\n",
        "\n",
        "        # compute internal derivatives\n",
        "        self.get_dxout_dxmid()\n",
        "        self.get_dxout_dbias()\n",
        "        self.get_dxmid_dw()\n",
        "        self.get_dxmid_dxin()\n",
        "\n",
        "        assert self.dxout_dxmid.shape == (self.n_in, )\n",
        "        assert self.dxmid_dxin.shape == (self.n_in, )\n",
        "        assert self.dxmid_dw.shape == (self.n_in, self.n_weights_per_edge)\n",
        "\n",
        "        # compute external derivatives\n",
        "        self.get_dxout_dxin()\n",
        "        self.get_dxout_dw()\n",
        "\n",
        "        return self.xout\n",
        "\n",
        "    def get_xmid(self):\n",
        "        # compute self.xmid\n",
        "        pass\n",
        "\n",
        "    def get_xout(self):\n",
        "        # compute self.xout\n",
        "        pass\n",
        "\n",
        "    def get_dxout_dxmid(self):\n",
        "        # compute self.dxout_dxmid\n",
        "        pass\n",
        "\n",
        "    def get_dxout_dbias(self):\n",
        "        # compute self.dxout_dbias\n",
        "        pass  #self.dxout_dbias = 0  # by default\n",
        "\n",
        "    def get_dxmid_dw(self):\n",
        "        # compute self.dxmid_dw\n",
        "        pass\n",
        "\n",
        "    def get_dxmid_dxin(self):\n",
        "        # compute self.dxmid_dxin\n",
        "        pass\n",
        "\n",
        "    def get_dxout_dxin(self):\n",
        "        self.dxout_dxin = self.dxout_dxmid * self.dxmid_dxin\n",
        "\n",
        "    def get_dxout_dw(self):\n",
        "        self.dxout_dw = np.diag(self.dxout_dxmid) @ self.dxmid_dw\n",
        "\n",
        "    def update_dloss_dw_dbias(self, dloss_dxout):\n",
        "        self.dloss_dw += self.dxout_dw * dloss_dxout\n",
        "        self.dloss_dbias += self.dxout_dbias * dloss_dxout\n",
        "\n",
        "    def gradient_descent(self, eps):\n",
        "        self.weights -= eps * self.dloss_dw\n",
        "        self.bias -= eps * self.dloss_dbias"
      ],
      "metadata": {
        "id": "r026fn5zdIGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego se definen los métodos para las funciones de Activación:"
      ],
      "metadata": {
        "id": "N5Hk0YlAf2FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def relu(x, get_derivative=False):\n",
        "    return x * (x > 0) if not get_derivative else 1.0 * (x >= 0)\n",
        "\n",
        "def tanh_act(x, get_derivative=False):\n",
        "    if not get_derivative:\n",
        "        return math.tanh(x)\n",
        "    return 1 - math.tanh(x) ** 2\n",
        "\n",
        "def sigmoid_act(x, get_derivative=False):\n",
        "    if not get_derivative:\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "    return sigmoid_act(x) * (1 - sigmoid_act(x))"
      ],
      "metadata": {
        "id": "BzSVEyTQf6zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego se define la clase NeuronNN que implementa los métodos de MLP. Esta clase se inicializa con el número de entradas **(n_in)**, un rango que es opcional para los pesos **(weights_range)** y una función de activación **(activation)**. Por defecto, la función de activación es **ReLU**."
      ],
      "metadata": {
        "id": "BAIdAkTnhDBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuronNN(Neuron):\n",
        "\n",
        "    def __init__(self, n_in, weights_range=None, activation=relu):\n",
        "        super().__init__(n_in, n_weights_per_edge=1, weights_range=weights_range)\n",
        "        self.activation = activation\n",
        "        self.activation_input = None\n",
        "\n",
        "    def get_xmid(self):\n",
        "        self.xmid = self.weights[:, 0] * self.xin\n",
        "\n",
        "    def get_xout(self):\n",
        "        self.activation_input = sum(self.xmid.flatten()) + self.bias\n",
        "        self.xout = self.activation(self.activation_input, get_derivative=False)\n",
        "\n",
        "    def get_dxout_dxmid(self):\n",
        "        self.dxout_dxmid = self.activation(self.activation_input, get_derivative=True) * np.ones(self.n_in)\n",
        "\n",
        "    def get_dxout_dbias(self):\n",
        "        self.dxout_dbias = self.activation(self.activation_input, get_derivative=True)\n",
        "\n",
        "    def get_dxmid_dw(self):\n",
        "        self.dxmid_dw = np.reshape(self.xin, (-1, 1))\n",
        "\n",
        "    def get_dxmid_dxin(self):\n",
        "        self.dxmid_dxin = self.weights.flatten()"
      ],
      "metadata": {
        "id": "vKN6rDKnhDdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, se define la función **get_bsplines()** para genera un conjunto de funciones borde no lineales (B-splines) y sus derivadas asociadas que serán utilizadas en la posterior implementación de neuronas KAN."
      ],
      "metadata": {
        "id": "_KWPZm-QiJar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import BSpline\n",
        "\n",
        "def get_bsplines(x_bounds, n_fun, degree=3, **kwargs):\n",
        "    grid_len = n_fun - degree + 1\n",
        "    step = (x_bounds[1] - x_bounds[0]) / (grid_len - 1)\n",
        "    edge_fun, edge_fun_der = {}, {}\n",
        "\n",
        "    # SiLU bias function\n",
        "    edge_fun[0] = lambda x: x / (1 + np.exp(-x))\n",
        "    edge_fun_der[0] = lambda x: (1 + np.exp(-x) + x * np.exp(-x)) / np.power((1 + np.exp(-x)), 2)\n",
        "\n",
        "    # B-splines\n",
        "    t = np.linspace(x_bounds[0] - degree * step, x_bounds[1] + degree * step, grid_len + 2 * degree)\n",
        "    t[degree], t[-degree - 1] = x_bounds[0], x_bounds[1]\n",
        "    for ind_spline in range(n_fun - 1):\n",
        "        edge_fun[ind_spline + 1] = BSpline.basis_element(t[ind_spline:ind_spline + degree + 2], extrapolate=False)\n",
        "        edge_fun_der[ind_spline + 1] = edge_fun[ind_spline + 1].derivative()\n",
        "    return edge_fun, edge_fun_der"
      ],
      "metadata": {
        "id": "3r8SUis2iMlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de esto, se puede hacer la clase NeuronKAN:"
      ],
      "metadata": {
        "id": "Z1W94hz7jVXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuronKAN(Neuron):\n",
        "\n",
        "    def __init__(self, n_in, n_weights_per_edge, x_bounds, weights_range=None, get_edge_fun=get_bsplines, **kwargs):\n",
        "        self.x_bounds = x_bounds\n",
        "        super().__init__(n_in, n_weights_per_edge=n_weights_per_edge, weights_range=weights_range)\n",
        "        self.edge_fun, self.edge_fun_der = get_edge_fun(self.x_bounds, self.n_weights_per_edge, **kwargs)\n",
        "\n",
        "    def get_xmid(self):\n",
        "        # apply edge functions\n",
        "        self.phi_x_mat = np.array([self.edge_fun[b](self.xin) for b in self.edge_fun]).T\n",
        "        self.phi_x_mat[np.isnan(self.phi_x_mat)] = 0\n",
        "        self.xmid = (self.weights * self.phi_x_mat).sum(axis=1)\n",
        "\n",
        "    def get_xout(self):\n",
        "        # note: node function <- tanh to avoid any update of spline grids\n",
        "        self.xout = tanh_act(sum(self.xmid.flatten()), get_derivative=False)\n",
        "\n",
        "    def get_dxout_dxmid(self):\n",
        "        self.dxout_dxmid = tanh_act(sum(self.xmid.flatten()), get_derivative=True) * np.ones(self.n_in)\n",
        "\n",
        "    def get_dxmid_dw(self):\n",
        "        self.dxmid_dw = self.phi_x_mat\n",
        "\n",
        "    def get_dxmid_dxin(self):\n",
        "        phi_x_der_mat = np.array([self.edge_fun_der[b](self.xin) if self.edge_fun[b](self.xin) is not None else 0\n",
        "                                  for b in self.edge_fun_der]).T  # shape (n_in, n_weights_per_edge)\n",
        "        phi_x_der_mat[np.isnan(phi_x_der_mat)] = 0\n",
        "        self.dxmid_dxin = (self.weights * phi_x_der_mat).sum(axis=1)\n",
        "\n",
        "    def get_dxout_dbias(self):\n",
        "        # no bias in KAN!\n",
        "        self.dxout_dbias = 0"
      ],
      "metadata": {
        "id": "D_F3v7Mnkgvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 2\n"
      ],
      "metadata": {
        "id": "eilrhphpbT5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtenga resultados de ejecución de las ANN como en el tutorial. En el punto 5 del tutorial, modifique los datos de entrada usando otros datasets y vuelva a hacer comparaciones entre los resultados obtenidos de las distintas redes usadas en el tutorial"
      ],
      "metadata": {
        "id": "HDZ7HaWZbh92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 3\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MknekV5RbVif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explique en sus propias palabras cómo está construida la arquitectura KAN y por qué funciona. Además, haga un análisis sobre los resultados obtenidos y cómo y por qué podrían variar."
      ],
      "metadata": {
        "id": "oyyuJnK5bqe3"
      }
    }
  ]
}