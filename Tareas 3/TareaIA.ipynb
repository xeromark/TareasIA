{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA5BN/jQxVXqkliRcWq1KK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xeromark/TareasIA/blob/main/Tareas%203/TareaIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tarea 3 de IA de Omar Marca y Luis Reyes. Github: https://github.com/xeromark/TareasIA\n",
        "\n"
      ],
      "metadata": {
        "id": "DJLWWciOcCSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 1"
      ],
      "metadata": {
        "id": "k80rj0X2bOmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reproduzca el tutorial implementando la arquitectura. Comente paso a paso lo que se hace en sus propias palabras."
      ],
      "metadata": {
        "id": "G-CM64tLbZVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las neuronas KAN utilizan funciones de borde no lineales para transformar las entradas con el objetivo de que se pueda procesar la información con menos recursos computacionales y realizar cálculos más complejos.\n",
        "\n",
        "\n",
        "\n",
        "Por otra parte, en MLP las entradas se transforman mediante funciones de activación lineales o no lineales para introducir no linealidades en el modelo, es decir, que el modelo sea capaz de representar relaciones complejas entre las variables de entrada y salida que no pueden ser descritas por una simple relación lineal.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aRWSl4O4b6Nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neurona\n",
        "\n",
        "Para poder utilizar MLP y KAN, se establecen métodos en python para proporcionar una estructura versátil que pueda acomodar tanto las Redes de Kolmogorov-Arnold (KAN) como los Perceptrones Multicapa (MLP). Es por ello que se tienen los siguientes métodos del funcionamiento de una neurona:"
      ],
      "metadata": {
        "id": "qlW_KivkoITc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "    def __init__(self, n_in, n_weights_per_edge, weights_range=None):\n",
        "        self.n_in = n_in  # n. inputs\n",
        "        self.n_weights_per_edge = n_weights_per_edge\n",
        "        weights_range = [-1, 1] if weights_range is None else weights_range\n",
        "        self.weights = np.random.uniform(weights_range[0], weights_range[-1], size=(self.n_in, self.n_weights_per_edge))\n",
        "        self.bias = 0\n",
        "        self.xin = None  # input variable\n",
        "        self.xmid = None  # edge variables\n",
        "        self.xout = None  # output variable\n",
        "        self.dxout_dxmid = None  # derivative d xout / d xmid: (n_in, )\n",
        "        self.dxout_dbias = None  # derivative d xout / d bias\n",
        "        self.dxmid_dw = None  # derivative d xmid / d w: (n_in, n_par_per_edge)\n",
        "        self.dxmid_dxin = None  # derivative d xmid / d xin\n",
        "        self.dxout_dxin = None  # (composite) derivative d xout / d xin\n",
        "        self.dxout_dw = None  # (composite) derivative d xout / d w\n",
        "        self.dloss_dw = np.zeros((self.n_in, self.n_weights_per_edge))  # (composite) derivative d loss / d w\n",
        "        self.dloss_dbias = 0  # (composite) derivative d loss / d bias\n",
        "\n",
        "    def __call__(self, xin):\n",
        "        # forward pass: compute neuron's output\n",
        "        self.xin = np.array(xin)\n",
        "        self.get_xmid()\n",
        "        self.get_xout()\n",
        "\n",
        "        # compute internal derivatives\n",
        "        self.get_dxout_dxmid()\n",
        "        self.get_dxout_dbias()\n",
        "        self.get_dxmid_dw()\n",
        "        self.get_dxmid_dxin()\n",
        "\n",
        "        assert self.dxout_dxmid.shape == (self.n_in, )\n",
        "        assert self.dxmid_dxin.shape == (self.n_in, )\n",
        "        assert self.dxmid_dw.shape == (self.n_in, self.n_weights_per_edge)\n",
        "\n",
        "        # compute external derivatives\n",
        "        self.get_dxout_dxin()\n",
        "        self.get_dxout_dw()\n",
        "\n",
        "        return self.xout\n",
        "\n",
        "    def get_xmid(self):\n",
        "        # compute self.xmid\n",
        "        pass\n",
        "\n",
        "    def get_xout(self):\n",
        "        # compute self.xout\n",
        "        pass\n",
        "\n",
        "    def get_dxout_dxmid(self):\n",
        "        # compute self.dxout_dxmid\n",
        "        pass\n",
        "\n",
        "    def get_dxout_dbias(self):\n",
        "        # compute self.dxout_dbias\n",
        "        pass  #self.dxout_dbias = 0  # by default\n",
        "\n",
        "    def get_dxmid_dw(self):\n",
        "        # compute self.dxmid_dw\n",
        "        pass\n",
        "\n",
        "    def get_dxmid_dxin(self):\n",
        "        # compute self.dxmid_dxin\n",
        "        pass\n",
        "\n",
        "    def get_dxout_dxin(self):\n",
        "        self.dxout_dxin = self.dxout_dxmid * self.dxmid_dxin\n",
        "\n",
        "    def get_dxout_dw(self):\n",
        "        self.dxout_dw = np.diag(self.dxout_dxmid) @ self.dxmid_dw\n",
        "\n",
        "    def update_dloss_dw_dbias(self, dloss_dxout):\n",
        "        self.dloss_dw += self.dxout_dw * dloss_dxout\n",
        "        self.dloss_dbias += self.dxout_dbias * dloss_dxout\n",
        "\n",
        "    def gradient_descent(self, eps):\n",
        "        self.weights -= eps * self.dloss_dw\n",
        "        self.bias -= eps * self.dloss_dbias"
      ],
      "metadata": {
        "id": "r026fn5zdIGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función de activación\n",
        "\n",
        "Para poder facilitar el uso de las funciones de Activación, se definen los siguientes métodos:"
      ],
      "metadata": {
        "id": "N5Hk0YlAf2FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def relu(x, get_derivative=False):\n",
        "    return x * (x > 0) if not get_derivative else 1.0 * (x >= 0)\n",
        "\n",
        "def tanh_act(x, get_derivative=False):\n",
        "    if not get_derivative:\n",
        "        return math.tanh(x)\n",
        "    return 1 - math.tanh(x) ** 2\n",
        "\n",
        "def sigmoid_act(x, get_derivative=False):\n",
        "    if not get_derivative:\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "    return sigmoid_act(x) * (1 - sigmoid_act(x))"
      ],
      "metadata": {
        "id": "BzSVEyTQf6zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP Métodos\n",
        "\n",
        "Luego se define la clase NeuronNN que implementa los métodos de MLP. Esta clase se inicializa con el número de entradas **(n_in)**, un rango que es opcional para los pesos **(weights_range)** y una función de activación **(activation)**. Por defecto, la función de activación es **ReLU**."
      ],
      "metadata": {
        "id": "BAIdAkTnhDBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuronNN(Neuron):\n",
        "\n",
        "    def __init__(self, n_in, weights_range=None, activation=relu):\n",
        "        super().__init__(n_in, n_weights_per_edge=1, weights_range=weights_range)\n",
        "        self.activation = activation\n",
        "        self.activation_input = None\n",
        "\n",
        "    def get_xmid(self):\n",
        "        self.xmid = self.weights[:, 0] * self.xin\n",
        "\n",
        "    def get_xout(self):\n",
        "        self.activation_input = sum(self.xmid.flatten()) + self.bias\n",
        "        self.xout = self.activation(self.activation_input, get_derivative=False)\n",
        "\n",
        "    def get_dxout_dxmid(self):\n",
        "        self.dxout_dxmid = self.activation(self.activation_input, get_derivative=True) * np.ones(self.n_in)\n",
        "\n",
        "    def get_dxout_dbias(self):\n",
        "        self.dxout_dbias = self.activation(self.activation_input, get_derivative=True)\n",
        "\n",
        "    def get_dxmid_dw(self):\n",
        "        self.dxmid_dw = np.reshape(self.xin, (-1, 1))\n",
        "\n",
        "    def get_dxmid_dxin(self):\n",
        "        self.dxmid_dxin = self.weights.flatten()"
      ],
      "metadata": {
        "id": "vKN6rDKnhDdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, se define la función **get_bsplines()** para genera un conjunto de funciones borde no lineales (B-splines) y sus derivadas asociadas que serán utilizadas en la posterior implementación de neuronas KAN."
      ],
      "metadata": {
        "id": "_KWPZm-QiJar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import BSpline\n",
        "\n",
        "def get_bsplines(x_bounds, n_fun, degree=3, **kwargs):\n",
        "    grid_len = n_fun - degree + 1\n",
        "    step = (x_bounds[1] - x_bounds[0]) / (grid_len - 1)\n",
        "    edge_fun, edge_fun_der = {}, {}\n",
        "\n",
        "    # SiLU bias function\n",
        "    edge_fun[0] = lambda x: x / (1 + np.exp(-x))\n",
        "    edge_fun_der[0] = lambda x: (1 + np.exp(-x) + x * np.exp(-x)) / np.power((1 + np.exp(-x)), 2)\n",
        "\n",
        "    # B-splines\n",
        "    t = np.linspace(x_bounds[0] - degree * step, x_bounds[1] + degree * step, grid_len + 2 * degree)\n",
        "    t[degree], t[-degree - 1] = x_bounds[0], x_bounds[1]\n",
        "    for ind_spline in range(n_fun - 1):\n",
        "        edge_fun[ind_spline + 1] = BSpline.basis_element(t[ind_spline:ind_spline + degree + 2], extrapolate=False)\n",
        "        edge_fun_der[ind_spline + 1] = edge_fun[ind_spline + 1].derivative()\n",
        "    return edge_fun, edge_fun_der"
      ],
      "metadata": {
        "id": "3r8SUis2iMlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clase KAN\n",
        "\n",
        "Ya teniendo todas las funciones y métodos de python necesarios para armar la clase para KAN en python, se obtiene lo siguiente:"
      ],
      "metadata": {
        "id": "Z1W94hz7jVXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuronKAN(Neuron):\n",
        "\n",
        "    def __init__(self, n_in, n_weights_per_edge, x_bounds, weights_range=None, get_edge_fun=get_bsplines, **kwargs):\n",
        "        self.x_bounds = x_bounds\n",
        "        super().__init__(n_in, n_weights_per_edge=n_weights_per_edge, weights_range=weights_range)\n",
        "        self.edge_fun, self.edge_fun_der = get_edge_fun(self.x_bounds, self.n_weights_per_edge, **kwargs)\n",
        "\n",
        "    def get_xmid(self):\n",
        "        # apply edge functions\n",
        "        self.phi_x_mat = np.array([self.edge_fun[b](self.xin) for b in self.edge_fun]).T\n",
        "        self.phi_x_mat[np.isnan(self.phi_x_mat)] = 0\n",
        "        self.xmid = (self.weights * self.phi_x_mat).sum(axis=1)\n",
        "\n",
        "    def get_xout(self):\n",
        "        # note: node function <- tanh to avoid any update of spline grids\n",
        "        self.xout = tanh_act(sum(self.xmid.flatten()), get_derivative=False)\n",
        "\n",
        "    def get_dxout_dxmid(self):\n",
        "        self.dxout_dxmid = tanh_act(sum(self.xmid.flatten()), get_derivative=True) * np.ones(self.n_in)\n",
        "\n",
        "    def get_dxmid_dw(self):\n",
        "        self.dxmid_dw = self.phi_x_mat\n",
        "\n",
        "    def get_dxmid_dxin(self):\n",
        "        phi_x_der_mat = np.array([self.edge_fun_der[b](self.xin) if self.edge_fun[b](self.xin) is not None else 0\n",
        "                                  for b in self.edge_fun_der]).T  # shape (n_in, n_weights_per_edge)\n",
        "        phi_x_der_mat[np.isnan(phi_x_der_mat)] = 0\n",
        "        self.dxmid_dxin = (self.weights * phi_x_der_mat).sum(axis=1)\n",
        "\n",
        "    def get_dxout_dbias(self):\n",
        "        # no bias in KAN!\n",
        "        self.dxout_dbias = 0"
      ],
      "metadata": {
        "id": "D_F3v7Mnkgvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Capas totalmente conectadas\n",
        "\n",
        "\n",
        "Una capa totalmente conectada es una colección de neuronas donde el mismo vector de entrada se alimenta a todas las neuronas de la capa. La salida de la capa es un vector que contiene las salidas de todas las neuronas. A continuación se presenta su implementación en python:"
      ],
      "metadata": {
        "id": "StLhpwivIIKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnectedLayer:\n",
        "\n",
        "    def __init__(self, n_in, n_out, neuron_class=NeuronNN, **kwargs):\n",
        "        self.n_in, self.n_out = n_in, n_out\n",
        "        self.neurons = [neuron_class(n_in) if (kwargs == {}) else neuron_class(n_in, **kwargs) for _ in range(n_out)]\n",
        "        self.xin = None  # input, shape (n_in,)\n",
        "        self.xout = None  # output, shape (n_out,)\n",
        "        self.dloss_dxin = None  # d loss / d xin, shape (n_in,)\n",
        "        self.zero_grad()\n",
        "\n",
        "    def __call__(self, xin):\n",
        "        # forward pass\n",
        "        self.xin = xin\n",
        "        self.xout = np.array([nn(self.xin) for nn in self.neurons])\n",
        "        return self.xout\n",
        "\n",
        "    def zero_grad(self, which=None):\n",
        "        # reset gradients to zero\n",
        "        if which is None:\n",
        "            which = ['xin', 'weights', 'bias']\n",
        "        for w in which:\n",
        "            if w == 'xin':  # reset layer's d loss / d xin\n",
        "                self.dloss_dxin = np.zeros(self.n_in)\n",
        "            elif w == 'weights':  # reset d loss / dw to zero for every neuron\n",
        "                for nn in self.neurons:\n",
        "                    nn.dloss_dw = np.zeros((self.n_in, self.neurons[0].n_weights_per_edge))\n",
        "            elif w == 'bias':  # reset d loss / db to zero for every neuron\n",
        "                for nn in self.neurons:\n",
        "                    nn.dloss_dbias = 0\n",
        "            else:\n",
        "                raise ValueError('input \\'which\\' value not recognized')\n",
        "\n",
        "    def update_grad(self, dloss_dxout):\n",
        "        # update gradients by chain rule\n",
        "        for ii, dloss_dxout_tmp in enumerate(dloss_dxout):\n",
        "            # update layer's d loss / d xin via chain rule\n",
        "            # note: account for all possible xin -> xout -> loss paths!\n",
        "            self.dloss_dxin += self.neurons[ii].dxout_dxin * dloss_dxout_tmp\n",
        "            # update neuron's d loss / dw and d loss / d bias\n",
        "            self.neurons[ii].update_dloss_dw_dbias(dloss_dxout_tmp)\n",
        "        return self.dloss_dxin"
      ],
      "metadata": {
        "id": "NF_HGCgspuWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Función de pérdida\n",
        "\n",
        "Para saber que tan bueno es el modelo, se deben evalúa qué tan cerca se alinean las predicciones de la red respecto a la salida con la que se está entrenando **(y_train)** con una función de pérdida. Es por ello que a continuación se presenta la implementación en python:"
      ],
      "metadata": {
        "id": "0cfnT2Zdqwfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "\n",
        "    def __init__(self, n_in):\n",
        "        self.n_in = n_in\n",
        "        self.y, self.dloss_dy, self.loss, self.y_train = None, None, None, None\n",
        "\n",
        "    def __call__(self, y, y_train):\n",
        "        # y: output of network\n",
        "        # y_train: ground truth\n",
        "        self.y, self.y_train = np.array(y), y_train\n",
        "        self.get_loss()\n",
        "        self.get_dloss_dy()\n",
        "        return self.loss\n",
        "\n",
        "    def get_loss(self):\n",
        "        # compute loss l(y, y_train)\n",
        "        pass\n",
        "\n",
        "    def get_dloss_dy(self):\n",
        "        # compute gradient of loss wrt y\n",
        "        pass\n",
        "\n",
        "\n",
        "class SquaredLoss(Loss):\n",
        "\n",
        "    def get_loss(self):\n",
        "        # compute loss l(xin, y)\n",
        "        self.loss = np.mean(np.power(self.y - self.y_train, 2))\n",
        "\n",
        "    def get_dloss_dy(self):\n",
        "        # compute gradient of loss wrt xin\n",
        "        self.dloss_dy = 2 * (self.y - self.y_train) / self.n_in\n",
        "\n",
        "\n",
        "class CrossEntropyLoss(Loss):\n",
        "\n",
        "    def get_loss(self):\n",
        "        # compute loss l(xin, y)\n",
        "        self.loss = - np.log(np.exp(self.y[self.y_train[0]]) / sum(np.exp(self.y)))\n",
        "\n",
        "    def get_dloss_dy(self):\n",
        "        # compute gradient of loss wrt xin\n",
        "        self.dloss_dy = np.exp(self.y) / sum(np.exp(self.y))\n",
        "        self.dloss_dy[self.y_train] -= 1"
      ],
      "metadata": {
        "id": "rzgHUnrUra9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Red Feed-Forward"
      ],
      "metadata": {
        "id": "Syrms1t0robw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una red feed-forward se usa para transformar las entradas en salidas útiles mediante una serie de capas de neuronas totalmente conectadas."
      ],
      "metadata": {
        "id": "Ls3tBNnKr24A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class FeedForward:\n",
        "    def __init__(self, layer_len, eps=.0001, seed=None, loss=SquaredLoss, **kwargs):\n",
        "        self.seed = np.random.randint(int(1e4)) if seed is None else int(seed)\n",
        "        np.random.seed(self.seed)\n",
        "        self.layer_len = layer_len\n",
        "        self.eps = eps\n",
        "        self.n_layers = len(self.layer_len) - 1\n",
        "        self.layers = [FullyConnectedLayer(layer_len[ii], layer_len[ii + 1], **kwargs) for ii in range(self.n_layers)]\n",
        "        self.loss = loss(self.layer_len[-1])\n",
        "        self.loss_hist = None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # forward pass\n",
        "        x_in = x\n",
        "        for ll in range(self.n_layers):\n",
        "            x_in = self.layers[ll](x_in)\n",
        "        return x_in\n",
        "\n",
        "    def backprop(self):\n",
        "        # gradient backpropagation\n",
        "        delta = self.layers[-1].update_grad(self.loss.dloss_dy)\n",
        "        for ll in range(self.n_layers - 1)[::-1]:\n",
        "            delta = self.layers[ll].update_grad(delta)\n",
        "\n",
        "    def gradient_descent_par(self):\n",
        "        # update parameters via gradient descent\n",
        "        for ll in self.layers:\n",
        "            for nn in ll.neurons:\n",
        "                nn.gradient_descent(self.eps)\n",
        "\n",
        "    def train(self, x_train, y_train, n_iter_max=10000, loss_tol=.1):\n",
        "        self.loss_hist = np.zeros(n_iter_max)\n",
        "        x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "        assert x_train.shape[0] == y_train.shape[0], 'x_train, y_train must contain the same number of samples'\n",
        "        assert x_train.shape[1] == self.layer_len[0], 'shape of x_train is incompatible with first layer'\n",
        "\n",
        "        pbar = tqdm(range(n_iter_max))\n",
        "        for it in pbar:\n",
        "            loss = 0  # reset loss\n",
        "            for ii in range(x_train.shape[0]):\n",
        "                x_out = self(x_train[ii, :])  # forward pass\n",
        "                loss += self.loss(x_out, y_train[ii, :])  # accumulate loss\n",
        "                self.backprop()  # backward propagation\n",
        "                [layer.zero_grad(which=['xin']) for layer in self.layers]  # reset gradient wrt xin to zero\n",
        "            self.loss_hist[it] = loss\n",
        "            if (it % 10) == 0:\n",
        "                pbar.set_postfix_str(f'loss: {loss:.3f}')  #\n",
        "            if loss < loss_tol:\n",
        "                pbar.set_postfix_str(f'loss: {loss:.3f}. Convergence has been attained!')\n",
        "                self.loss_hist = self.loss_hist[: it]\n",
        "                break\n",
        "            self.gradient_descent_par()  # update parameters\n",
        "            [layer.zero_grad(which=['weights', 'bias']) for layer in self.layers]  # reset gradient wrt par to zero"
      ],
      "metadata": {
        "id": "WMYNVegCr1Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Implementación de KAN y MLP"
      ],
      "metadata": {
        "id": "mttyhLExswBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from feed_forward_network.feedforward import FeedForward\n",
        "from neuron.neuron_nn import NeuronNN\n",
        "from neuron.neuron_kan import NeuronKAN\n",
        "import matplotlib.pyplot as plt\n",
        "from utils.activations import tanh_act, relu, sigmoid_act\n",
        "from utils.edge_fun import get_bsplines\n",
        "\n",
        "folder_fig = './'\n",
        "\n",
        "color_plots = {'dataset': 'b',\n",
        "               'kan': 'orange',\n",
        "               'mlp': 'green'}\n",
        "\n",
        "\n",
        "x_train = np.linspace(-1, 1, 50).reshape(-1, 1)\n",
        "y_train = .5 * np.sin(4 * x_train) * np.exp(-(x_train+1)) + .5  # damped sinusoid\n",
        "\n",
        "n_iter_train_1d = 500\n",
        "loss_tol_1d = .05\n",
        "seed = 476\n",
        "\n",
        "\n",
        "kan_1d = FeedForward([1, 2, 2, 1],  # layer size\n",
        "                  eps=.01,  # gradient descent parameter\n",
        "                  n_weights_per_edge=7,  # n. edge functions\n",
        "                  neuron_class=NeuronKAN,\n",
        "                  x_bounds=[-1, 1],  # input domain bounds\n",
        "                  get_edge_fun=get_bsplines,  # edge function type (B-splines ot Chebyshev)\n",
        "                  seed=seed,\n",
        "                  weights_range=[-1, 1])\n",
        "kan_1d.train(x_train,\n",
        "          y_train,\n",
        "          n_iter_max=n_iter_train_1d,\n",
        "          loss_tol=loss_tol_1d)\n",
        "\n",
        "\n",
        "mlp_1d = FeedForward([1, 13, 1],  # layer size\n",
        "                  eps=.005,  # gradient descend parameter\n",
        "                  activation=relu,  # activation type (ReLU, tanh or sigmoid)\n",
        "                  neuron_class=NeuronNN,\n",
        "                  seed=seed,\n",
        "                  weights_range=[-.5, .5])\n",
        "mlp_1d.train(x_train,\n",
        "             y_train,\n",
        "             n_iter_max=n_iter_train_1d,\n",
        "             loss_tol=loss_tol_1d)\n",
        "\n",
        "\n",
        "\n",
        "# Regression on training data\n",
        "fig, ax = plt.subplots(figsize=(4,3.2))\n",
        "x_plot = np.linspace(x_train[0], x_train[-1], 1000).reshape(-1, 1)\n",
        "ax.plot(x_train, y_train, 'o', color=color_plots['dataset'], label='training dataset')\n",
        "ax.plot(x_plot, [kan_1d(x) for x in x_plot], color=color_plots['kan'], label='KAN')\n",
        "ax.plot(x_train, [kan_1d(x) for x in x_train], 'x', color=color_plots['kan'], fillstyle='none')\n",
        "ax.plot(x_plot, [mlp_1d(x) for x in x_plot], color=color_plots['mlp'], label='MLP')\n",
        "ax.plot(x_train, [mlp_1d(x) for x in x_train], 'd', color=color_plots['mlp'], fillstyle='none')\n",
        "ax.set_xlabel('input feature', fontsize=13)\n",
        "ax.set_title('Regression', fontsize=15)\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "#fig.savefig(folder_fig + 'regr1D.png', dpi=500)"
      ],
      "metadata": {
        "id": "auGMhnSts8fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 2\n"
      ],
      "metadata": {
        "id": "eilrhphpbT5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtenga resultados de ejecución de las ANN como en el tutorial. En el punto 5 del tutorial, modifique los datos de entrada usando otros datasets y vuelva a hacer comparaciones entre los resultados obtenidos de las distintas redes usadas en el tutorial"
      ],
      "metadata": {
        "id": "HDZ7HaWZbh92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 3\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MknekV5RbVif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explique en sus propias palabras cómo está construida la arquitectura KAN y por qué funciona. Además, haga un análisis sobre los resultados obtenidos y cómo y por qué podrían variar."
      ],
      "metadata": {
        "id": "oyyuJnK5bqe3"
      }
    }
  ]
}